{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "658ae657-79dc-4aaf-9e6d-aec2cf69e0ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Helper functions to implement delta change data feed read, predictive i/o (enable deletion vectors) for accelerated delta merge operations, custom delta merge and other operations in databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c950761-3fac-4a71-a9b8-cad3d01cd8a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enable_cdf_existing_table(table_name:str, table_path:str):\n",
    "    from pyspark.sql import DataFrame, SparkSession\n",
    "    from delta import DeltaTable\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Enable delta change data feed on existing data files and tables.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # enable cdf on new delta tables\n",
    "        spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "\n",
    "        # enable cdf on existing delta tables\n",
    "\n",
    "        # check if cdf already enabled\n",
    "        dt = DeltaTable.forPath(sparkSession=spark, path=table_path)\n",
    "        if \"delta.enableChangeDataFeed\" in str(dt.detail().select(col(\"properties\")).first()[\"properties\"]):\n",
    "            print(\"CDF is enabled\")\n",
    "        else:\n",
    "            # enable cdf on existing table by first dropping external table metadata, the data remains intact\n",
    "            spark.sql(f\"drop table if exists {table_name};\")\n",
    "\n",
    "            # create new delta table on existing data\n",
    "            spark.sql(f\"create external table if not exists {table_name} location '{table_path}'\")\n",
    "            spark.sql(f\"alter table {table_name} set tblproperties (delta.enableChangeDataFeed = true)\")\n",
    "            print(f\"A new table {table_name} has been created using existing location at `{table_path}`\")\n",
    "\n",
    "            if \"delta.enableChangeDataFeed\" in str(dt.detail().select(col(\"properties\")).first()[\"properties\"]):\n",
    "                print(f\" CDF has been successfully enable on table {table_name}\")\n",
    "    except AnalysisException as ae:\n",
    "        print(ae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c427e30b-2612-410a-b8f4-dcf826dc00f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enable_pio_existing_table(table_name:str, table_path:str):\n",
    "    # import packages\n",
    "    from pyspark.sql import DataFrame, SparkSession\n",
    "    from delta import DeltaTable\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Enable Predictive I/O (delta.enableDeletionVectors) on existing data files and tables.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # enable cdf on existing delta tables\n",
    "\n",
    "        # check if predictive io already enabled\n",
    "        dt = DeltaTable.forPath(sparkSession=spark, path=table_path)\n",
    "        if \"delta.enableDeletionVectors\" in str(dt.detail().select(col(\"properties\")).first()[\"properties\"]):\n",
    "            print(\"Predictive I/O is enabled\")\n",
    "        else:\n",
    "            # enable predictive io on existing table by first dropping external table metadata, the data remains intact\n",
    "            spark.sql(f\"drop table if exists {table_name};\")\n",
    "\n",
    "            # create new delta table on existing data\n",
    "            spark.sql(f\"create external table if not exists {table_name} location '{table_path}'\")\n",
    "            spark.sql(f\"alter table {table_name} set tblproperties (delta.enableDeletionVectors = true)\")\n",
    "            print(f\"A new table {table_name} has been created using existing location at `{table_path}`\")\n",
    "\n",
    "            if \"delta.enableDeletionVectors\" in str(dt.detail().select(col(\"properties\")).first()[\"properties\"]):\n",
    "                print(f\" Predictive I/O has been successfully enable on table {table_name}\")\n",
    "    except AnalysisException as ae:\n",
    "        print(ae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541607f0-072b-495b-92d2-7357dbd3e093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from delta import DeltaTable\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def merge_delta_table(df:DataFrame, target_tbl:str, primary_key_columns:Dict, target_tbl_path:str, func):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Custom function to implement delta merge on specified workload.\n",
    "    \"\"\"\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        try:\n",
    "            tbl = DeltaTable.forName(sparkSession=spark, tableOrViewName=target_tbl)\n",
    "            tbl_alias = \"target\"\n",
    "            df_alias = \"df_src\"\n",
    "            merge_condition = \" and \".join([f\"{tbl_alias}.{col} = {df_alias}.{col}\" for col in primary_key_columns])\n",
    "            tbl.alias(tbl_alias)\\\n",
    "            .merge(df.alias(df_alias),merge_condition)\\\n",
    "            .whenMatchedUpdateAll(condition = \"{df_alias}._change_type_ = update_postimage\")\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "        except:\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(target_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1279a949-df9d-4773-9672-227d92f54672",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, TimestampType, LongType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "import datetime\n",
    "\n",
    "def write_commit_version(write_path:str, commit_version_tbl_name:str, max_commit_version:LongType, max_commit_timestamp:TimestampType, table_name:str):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Write the current max commit version of the dataframe change data feed to a tracking log/table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        properties_list = []\n",
    "        schema = StructType([\n",
    "            StructField(\"Max_Commit_Version\", LongType(), True),\n",
    "            StructField(\"Max_Commit_Timestamp\", TimestampType(), True),\n",
    "            StructField(\"Process_Timestamp\", TimestampType(), True),\n",
    "            StructField(\"Table_Name\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        # initialize the variables\n",
    "        process_timestamp = datetime.datetime.now()\n",
    "        properties_list.append([max_commit_version, max_commit_timestamp, process_timestamp, table_name])\n",
    "\n",
    "        df_commit = spark.createDataFrame(data=properties_list, schema=schema)\n",
    "        df_commit.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", True).save(write_path)\n",
    "    except AnalysisException as ae:\n",
    "        print(ae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc8290a-c750-44c6-8a79-4e5bf2de3089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import datetime\n",
    "\n",
    "def get_starting_commit_version(tbl_name:str, commit_version_path:str):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Get current maximum commit version number from the commit version tracking log.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = spark.read.format(\"delta\").load(commit_version_path)\n",
    "        starting_version = df.select(max(df.Curr_Max_Commit_Ver)).first()[0]\n",
    "\n",
    "    except AnalysisException as ae:\n",
    "\n",
    "        print(ae)\n",
    "        starting_version = 0\n",
    "\n",
    "    return starting_version\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
